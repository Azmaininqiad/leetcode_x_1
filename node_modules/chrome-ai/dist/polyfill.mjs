var __defProp = Object.defineProperty;
var __defProps = Object.defineProperties;
var __getOwnPropDescs = Object.getOwnPropertyDescriptors;
var __getOwnPropSymbols = Object.getOwnPropertySymbols;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __propIsEnum = Object.prototype.propertyIsEnumerable;
var __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;
var __spreadValues = (a, b) => {
  for (var prop in b || (b = {}))
    if (__hasOwnProp.call(b, prop))
      __defNormalProp(a, prop, b[prop]);
  if (__getOwnPropSymbols)
    for (var prop of __getOwnPropSymbols(b)) {
      if (__propIsEnum.call(b, prop))
        __defNormalProp(a, prop, b[prop]);
    }
  return a;
};
var __spreadProps = (a, b) => __defProps(a, __getOwnPropDescs(b));
var __async = (__this, __arguments, generator) => {
  return new Promise((resolve, reject) => {
    var fulfilled = (value) => {
      try {
        step(generator.next(value));
      } catch (e) {
        reject(e);
      }
    };
    var rejected = (value) => {
      try {
        step(generator.throw(value));
      } catch (e) {
        reject(e);
      }
    };
    var step = (x) => x.done ? resolve(x.value) : Promise.resolve(x.value).then(fulfilled, rejected);
    step((generator = generator.apply(__this, __arguments)).next());
  });
};

// src/polyfill/session.ts
import { LlmInference } from "@mediapipe/tasks-genai";
import createDebug from "debug";
var debug = createDebug("chromeai:polyfill");
var PolyfillChromeAIAssistant = class {
  constructor(llm) {
    this.llm = llm;
    this.prompt = (prompt) => __async(this, null, function* () {
      const response = yield this.llm.generateResponse(prompt);
      debug("prompt", prompt, response);
      return response;
    });
    this.promptStreaming = (prompt) => {
      debug("promptStreaming", prompt);
      const stream = new ReadableStream({
        start: (controller) => {
          const listener = (partialResult, done) => {
            controller.enqueue(partialResult);
            if (done) {
              controller.close();
            }
          };
          this.llm.generateResponse(prompt, listener);
        },
        cancel: (reason) => {
          console.warn("stream text canceled", reason);
        }
      });
      debug("promptStreaming", prompt);
      return stream;
    };
    this.destroy = () => __async(this, null, function* () {
      return this.llm.close();
    });
    debug("PolyfillChromeAIAssistant created", llm);
  }
};
var PolyfillChromeAIAssistantFactory = class {
  constructor(aiOptions = {}) {
    this.aiOptions = {
      wasmBinaryPath: "https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/genai_wasm_internal.wasm",
      wasmLoaderPath: "https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/genai_wasm_internal.js",
      // About 1.78GB, should cache by browser
      modelAssetPath: "https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/gemini-nano-it-chrome-128.bin"
    };
    this.capabilities = () => __async(this, null, function* () {
      const defaultOptions = {
        defaultTemperature: 0.8,
        defaultTopK: 3,
        maxTopK: 128
      };
      if (typeof WebAssembly.instantiate !== "function")
        return __spreadProps(__spreadValues({}, defaultOptions), {
          available: "no" /* NO */
        });
      if (!navigator.gpu)
        return __spreadProps(__spreadValues({}, defaultOptions), {
          available: "no" /* NO */
        });
      const isModelAssetBufferReady = yield Promise.race([
        this.modelAssetBuffer,
        Promise.resolve("sentinel")
      ]).then((value) => value === "sentinel").catch(() => true);
      return __spreadProps(__spreadValues({}, defaultOptions), {
        available: isModelAssetBufferReady ? "readily" /* READILY */ : "after-download" /* AFTER_DOWNLOAD */
      });
    });
    this.create = (options) => __async(this, null, function* () {
      const defaultParams = yield this.capabilities();
      const argv = Object.assign(
        {
          temperature: defaultParams.defaultTemperature,
          topK: defaultParams.defaultTopK
        },
        options
      );
      const llm = yield LlmInference.createFromOptions(
        {
          wasmLoaderPath: this.aiOptions.wasmLoaderPath,
          wasmBinaryPath: this.aiOptions.wasmBinaryPath
        },
        {
          baseOptions: {
            modelAssetBuffer: yield this.modelAssetBuffer
          },
          temperature: argv.temperature,
          topK: argv.topK
        }
      );
      const session = new PolyfillChromeAIAssistant(llm);
      debug("createSession", options, session);
      return session;
    });
    this.aiOptions = Object.assign(this.aiOptions, aiOptions);
    debug("PolyfillChromeAI created", this.aiOptions);
    this.modelAssetBuffer = fetch(this.aiOptions.modelAssetPath).then(
      (response) => response.body.getReader()
    );
  }
};
var polyfillChromeAI = (options) => {
  const ai = {
    assistant: new PolyfillChromeAIAssistantFactory(options)
  };
  globalThis.ai = globalThis.ai || ai;
  globalThis.model = globalThis.model || ai;
};

// src/polyfill/index.ts
polyfillChromeAI(globalThis.__polyfill_ai_options__);
//# sourceMappingURL=polyfill.mjs.map