{"version":3,"sources":["../src/language-model.ts","../src/stream-ai.ts","../src/embedding-model.ts","../src/chromeai.ts","../src/polyfill/session.ts"],"sourcesContent":["import {\n  LanguageModelV1,\n  LanguageModelV1CallOptions,\n  LanguageModelV1CallWarning,\n  LanguageModelV1FinishReason,\n  LanguageModelV1FunctionToolCall,\n  LanguageModelV1ImagePart,\n  LanguageModelV1LogProbs,\n  LanguageModelV1Prompt,\n  LanguageModelV1StreamPart,\n  LanguageModelV1TextPart,\n  LanguageModelV1ToolCallPart,\n  LanguageModelV1ToolResultPart,\n  LoadSettingError,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport type {\n  ChromeAIAssistant,\n  ChromeAIAssistantCreateOptions,\n} from './global';\nimport { ChromeAICapabilityAvailability } from './enum';\nimport createDebug from 'debug';\nimport { objectStartSequence, objectStopSequence, StreamAI } from './stream-ai';\n\nconst debug = createDebug('chromeai');\n\nexport type ChromeAIChatModelId = 'text';\n\nexport interface ChromeAIChatSettings extends ChromeAIAssistantCreateOptions {}\n\nfunction getStringContent(\n  content:\n    | string\n    | (LanguageModelV1TextPart | LanguageModelV1ImagePart)[]\n    | (LanguageModelV1TextPart | LanguageModelV1ToolCallPart)[]\n    | LanguageModelV1ToolResultPart[]\n): string {\n  if (typeof content === 'string') {\n    return content.trim();\n  } else if (Array.isArray(content) && content.length > 0) {\n    const [first] = content;\n    if (first.type !== 'text') {\n      throw new UnsupportedFunctionalityError({ functionality: 'toolCall' });\n    }\n    return first.text.trim();\n  } else {\n    return '';\n  }\n}\n\nexport class ChromeAIChatLanguageModel implements LanguageModelV1 {\n  readonly specificationVersion = 'v1';\n  readonly defaultObjectGenerationMode = 'json';\n  readonly modelId: ChromeAIChatModelId = 'text';\n  readonly provider = 'gemini-nano';\n  readonly supportsImageUrls = false;\n  readonly supportsStructuredOutputs = false;\n\n  options: ChromeAIChatSettings;\n\n  constructor(\n    modelId: ChromeAIChatModelId,\n    options: ChromeAIChatSettings = {}\n  ) {\n    this.modelId = modelId;\n    this.options = options;\n    debug('init:', this.modelId);\n  }\n\n  private session!: ChromeAIAssistant;\n  private getSession = async (\n    options?: ChromeAIAssistantCreateOptions\n  ): Promise<ChromeAIAssistant> => {\n    if (!globalThis.ai?.assistant) {\n      throw new LoadSettingError({ message: 'Browser not support' });\n    }\n    if (this.session) return this.session;\n\n    const cap = await ai.assistant.capabilities();\n\n    if (cap.available !== ChromeAICapabilityAvailability.READILY) {\n      throw new LoadSettingError({ message: 'Built-in model not ready' });\n    }\n\n    this.options = {\n      temperature: cap.defaultTemperature,\n      topK: cap.defaultTopK,\n      ...this.options,\n      ...options,\n    };\n\n    this.session = await ai.assistant.create(this.options);\n\n    debug('session created:', this.session, this.options);\n    return this.session;\n  };\n\n  private formatMessages = (options: LanguageModelV1CallOptions): string => {\n    let prompt: LanguageModelV1Prompt = options.prompt;\n    debug('before format prompt:', prompt);\n\n    let result = '';\n\n    if (\n      // When the user supplied a prompt input, we don't transform it\n      options.inputFormat === 'prompt' &&\n      prompt.length === 1 &&\n      prompt[0].role === 'user' &&\n      prompt[0].content.length === 1 &&\n      prompt[0].content[0].type === 'text'\n    ) {\n      result += prompt[0].content[0].text;\n    } else {\n      // Use magic prompt for object-json mode\n      if (options.mode.type === 'object-json') {\n        prompt.unshift({\n          role: 'system',\n          content: `Throughout our conversation, always start your responses with \"{\" and end with \"}\", ensuring the output is a concise JSON object and strictly avoid including any comments, notes, explanations, or examples in your output.\\nFor instance, if the JSON schema is {\"type\":\"object\",\"properties\":{\"someKey\":{\"type\":\"string\"}},\"required\":[\"someKey\"],\"additionalProperties\":false,\"$schema\":\"http://json-schema.org/draft-07/schema#\"}, your response should immediately begin with \"{\" and strictly end with \"}\", following the format: {\"someKey\": \"someValue\"}.\\nAdhere to this format for all queries moving forward.`,\n        });\n      }\n\n      for (let index = 0; index < prompt.length; index += 1) {\n        const { role, content } = prompt[index];\n        const contentString = getStringContent(content);\n\n        switch (role) {\n          case 'system':\n            result += `${contentString}\\n`;\n            break;\n          case 'assistant':\n          case 'tool':\n            result += `model\\n${contentString}\\n`;\n            break;\n          case 'user':\n          default:\n            result += `user\\n${contentString}\\n`;\n            break;\n        }\n      }\n      result += `model\\n`;\n    }\n\n    debug('formated message:', result);\n    return result;\n  };\n\n  public doGenerate = async (\n    options: LanguageModelV1CallOptions\n  ): Promise<{\n    text?: string;\n    toolCalls?: LanguageModelV1FunctionToolCall[];\n    finishReason: LanguageModelV1FinishReason;\n    usage: { promptTokens: number; completionTokens: number };\n    rawCall: { rawPrompt: unknown; rawSettings: Record<string, unknown> };\n    rawResponse?: { headers?: Record<string, string> };\n    warnings?: LanguageModelV1CallWarning[];\n    logprobs?: LanguageModelV1LogProbs;\n  }> => {\n    debug('generate options:', options);\n\n    if (['regular', 'object-json'].indexOf(options.mode.type) < 0) {\n      throw new UnsupportedFunctionalityError({\n        functionality: `${options.mode.type} mode`,\n      });\n    }\n\n    const session = await this.getSession();\n    const message = this.formatMessages(options);\n    let text = await session.prompt(message);\n\n    if (options.mode.type === 'object-json') {\n      text = text.replace(new RegExp('^' + objectStartSequence, 'ig'), '');\n      text = text.replace(new RegExp(objectStopSequence + '$', 'ig'), '');\n    }\n\n    debug('generate result:', text);\n\n    return {\n      text,\n      finishReason: 'stop',\n      usage: { promptTokens: 0, completionTokens: 0 },\n      rawCall: { rawPrompt: options.prompt, rawSettings: this.options },\n    };\n  };\n\n  public doStream = async (\n    options: LanguageModelV1CallOptions\n  ): Promise<{\n    stream: ReadableStream<LanguageModelV1StreamPart>;\n    rawCall: { rawPrompt: unknown; rawSettings: Record<string, unknown> };\n    rawResponse?: { headers?: Record<string, string> };\n    warnings?: LanguageModelV1CallWarning[];\n  }> => {\n    debug('stream options:', options);\n\n    if (['regular', 'object-json'].indexOf(options.mode.type) < 0) {\n      throw new UnsupportedFunctionalityError({\n        functionality: `${options.mode.type} mode`,\n      });\n    }\n\n    const session = await this.getSession();\n    const message = this.formatMessages(options);\n    const promptStream = session.promptStreaming(message);\n    const transformStream = new StreamAI(options);\n    const stream = promptStream.pipeThrough(transformStream);\n\n    return {\n      stream,\n      rawCall: { rawPrompt: options.prompt, rawSettings: this.options },\n    };\n  };\n}\n","import {\n  LanguageModelV1CallOptions,\n  LanguageModelV1StreamPart,\n} from '@ai-sdk/provider';\nimport createDebug from 'debug';\n\nconst debug = createDebug('chromeai');\n\nexport const objectStartSequence = ' ```json\\n';\nexport const objectStopSequence = '\\n```';\n\nexport class StreamAI extends TransformStream<\n  string,\n  LanguageModelV1StreamPart\n> {\n  public constructor(options: LanguageModelV1CallOptions) {\n    let buffer = '';\n    let transforming = false;\n\n    const reset = () => {\n      buffer = '';\n      transforming = false;\n    };\n\n    super({\n      start: (controller) => {\n        reset();\n        if (!options.abortSignal) return;\n        options.abortSignal.addEventListener('abort', () => {\n          debug('streamText terminate by abortSignal');\n          controller.terminate();\n        });\n      },\n      transform: (chunk, controller) => {\n        if (options.mode.type === 'object-json') {\n          transforming =\n            chunk.startsWith(objectStartSequence) &&\n            !chunk.endsWith(objectStopSequence);\n          chunk = chunk.replace(\n            new RegExp('^' + objectStartSequence, 'ig'),\n            ''\n          );\n          chunk = chunk.replace(new RegExp(objectStopSequence + '$', 'ig'), '');\n        } else {\n          transforming = true;\n        }\n        const textDelta = chunk.replace(buffer, ''); // See: https://github.com/jeasonstudio/chrome-ai/issues/11\n        if (transforming) controller.enqueue({ type: 'text-delta', textDelta });\n        buffer = chunk;\n      },\n      flush: (controller) => {\n        controller.enqueue({\n          type: 'finish',\n          finishReason: 'stop',\n          usage: { completionTokens: 0, promptTokens: 0 },\n        });\n        debug('stream result:', buffer);\n      },\n    });\n  }\n}\n","import { EmbeddingModelV1, EmbeddingModelV1Embedding } from '@ai-sdk/provider';\nimport { TextEmbedder } from '@mediapipe/tasks-text';\n\nexport interface ChromeAIEmbeddingModelSettings {\n  /**\n   * An optional base path to specify the directory the Wasm files should be loaded from.\n   * @default 'https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/text_wasm_internal.js'\n   */\n  wasmLoaderPath?: string;\n  /**\n   * It's about 6mb before gzip.\n   * @default 'https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/text_wasm_internal.wasm'\n   */\n  wasmBinaryPath?: string;\n  /**\n   * The model path to the model asset file.\n   * It's about 6.1mb before gzip.\n   * @default 'https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/universal_sentence_encoder.tflite'\n   */\n  modelAssetPath?: string;\n  /**\n   * Whether to normalize the returned feature vector with L2 norm. Use this\n   * option only if the model does not already contain a native L2_NORMALIZATION\n   * TF Lite Op. In most cases, this is already the case and L2 norm is thus\n   * achieved through TF Lite inference.\n   * @default false\n   */\n  l2Normalize?: boolean;\n  /**\n   * Whether the returned embedding should be quantized to bytes via scalar\n   * quantization. Embeddings are implicitly assumed to be unit-norm and\n   * therefore any dimension is guaranteed to have a value in [-1.0, 1.0]. Use\n   * the l2_normalize option if this is not the case.\n   * @default false\n   */\n  quantize?: boolean;\n  /**\n   * Overrides the default backend to use for the provided model.\n   */\n  delegate?: 'CPU' | 'GPU';\n}\n\n// See more:\n// - https://github.com/google-ai-edge/mediapipe\n// - https://ai.google.dev/edge/mediapipe/solutions/text/text_embedder/web_js\nexport class ChromeAIEmbeddingModel implements EmbeddingModelV1<string> {\n  readonly specificationVersion = 'v1';\n  readonly provider = 'google-mediapipe';\n  readonly modelId: string = 'embedding';\n  readonly supportsParallelCalls = true;\n  readonly maxEmbeddingsPerCall = undefined;\n\n  private settings: ChromeAIEmbeddingModelSettings = {\n    wasmLoaderPath:\n      'https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/text_wasm_internal.js',\n    wasmBinaryPath:\n      'https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/text_wasm_internal.wasm',\n    modelAssetPath:\n      'https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/universal_sentence_encoder.tflite',\n    l2Normalize: false,\n    quantize: false,\n  };\n  private modelAssetBuffer!: Promise<ReadableStreamDefaultReader>;\n  private textEmbedder!: Promise<TextEmbedder>;\n\n  public constructor(settings: ChromeAIEmbeddingModelSettings = {}) {\n    this.settings = { ...this.settings, ...settings };\n    this.modelAssetBuffer = fetch(this.settings.modelAssetPath!).then(\n      (response) => response.body!.getReader()\n    )!;\n    this.textEmbedder = this.getTextEmbedder();\n  }\n\n  protected getTextEmbedder = async (): Promise<TextEmbedder> => {\n    return TextEmbedder.createFromOptions(\n      {\n        wasmBinaryPath: this.settings.wasmBinaryPath!,\n        wasmLoaderPath: this.settings.wasmLoaderPath!,\n      },\n      {\n        baseOptions: {\n          modelAssetBuffer: await this.modelAssetBuffer,\n          delegate: this.settings.delegate,\n        },\n        l2Normalize: this.settings.l2Normalize,\n        quantize: this.settings.quantize,\n      }\n    );\n  };\n\n  public doEmbed = async (options: {\n    values: string[];\n    abortSignal?: AbortSignal;\n  }): Promise<{\n    embeddings: Array<EmbeddingModelV1Embedding>;\n    rawResponse?: Record<PropertyKey, any>;\n  }> => {\n    // if (options.abortSignal) console.warn('abortSignal is not supported');\n    const embedder = await this.textEmbedder;\n    const embeddings = options.values.map((text) => {\n      const embedderResult = embedder.embed(text);\n      const [embedding] = embedderResult.embeddings;\n      return embedding?.floatEmbedding ?? [];\n    });\n    return { embeddings };\n  };\n}\n","import {\n  ChromeAIEmbeddingModel,\n  ChromeAIEmbeddingModelSettings,\n} from './embedding-model';\nimport {\n  ChromeAIChatLanguageModel,\n  ChromeAIChatModelId,\n  ChromeAIChatSettings,\n} from './language-model';\nimport createDebug from 'debug';\n\nconst debug = createDebug('chromeai');\n\n/**\n * Create a new ChromeAI model/embedding instance.\n * @param modelId 'text' | 'embedding'\n * @param settings Options for the model\n */\nexport function chromeai(\n  modelId?: ChromeAIChatModelId,\n  settings?: ChromeAIChatSettings\n): ChromeAIChatLanguageModel;\nexport function chromeai(\n  modelId: 'embedding',\n  settings?: ChromeAIEmbeddingModelSettings\n): ChromeAIEmbeddingModel;\nexport function chromeai(modelId: unknown = 'text', settings: unknown = {}) {\n  debug('create instance', modelId, settings);\n  if (modelId === 'embedding') {\n    return new ChromeAIEmbeddingModel(\n      settings as ChromeAIEmbeddingModelSettings\n    );\n  }\n  return new ChromeAIChatLanguageModel(\n    modelId as ChromeAIChatModelId,\n    settings as ChromeAIChatSettings\n  );\n}\n\n/** @deprecated use `chromeai('embedding'[, options])` */\nchromeai.embedding = (settings: ChromeAIEmbeddingModelSettings = {}) =>\n  new ChromeAIEmbeddingModel(settings);\n","import { LlmInference, ProgressListener } from '@mediapipe/tasks-genai';\nimport type {\n  ChromeAIAssistant,\n  ChromeAIAssistantFactory,\n  ChromeAIAssistantCapabilities,\n  ChromeAIAssistantCreateOptions,\n  PolyfillChromeAIOptions,\n} from '../global';\nimport { ChromeAICapabilityAvailability } from '../enum';\nimport createDebug from 'debug';\n\nconst debug = createDebug('chromeai:polyfill');\n\nclass PolyfillChromeAIAssistant implements ChromeAIAssistant {\n  public constructor(private llm: LlmInference) {\n    debug('PolyfillChromeAIAssistant created', llm);\n  }\n\n  public prompt = async (prompt: string): Promise<string> => {\n    const response = await this.llm.generateResponse(prompt);\n    debug('prompt', prompt, response);\n    return response;\n  };\n\n  public promptStreaming = (prompt: string): ReadableStream<string> => {\n    debug('promptStreaming', prompt);\n    const stream = new ReadableStream<string>({\n      start: (controller) => {\n        const listener: ProgressListener = (\n          partialResult: string,\n          done: boolean\n        ) => {\n          controller.enqueue(partialResult);\n          if (done) {\n            controller.close();\n          }\n        };\n        this.llm.generateResponse(prompt, listener);\n      },\n      cancel: (reason) => {\n        console.warn('stream text canceled', reason);\n      },\n    });\n    debug('promptStreaming', prompt);\n    return stream;\n  };\n\n  public destroy = async () => this.llm.close();\n}\n\n/**\n * Model: https://huggingface.co/oongaboongahacker/Gemini-Nano\n */\nexport class PolyfillChromeAIAssistantFactory\n  implements ChromeAIAssistantFactory\n{\n  private aiOptions: PolyfillChromeAIOptions = {\n    wasmBinaryPath:\n      'https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/genai_wasm_internal.wasm',\n    wasmLoaderPath:\n      'https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/genai_wasm_internal.js',\n    // About 1.78GB, should cache by browser\n    modelAssetPath:\n      'https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/gemini-nano-it-chrome-128.bin',\n  };\n\n  public constructor(aiOptions: Partial<PolyfillChromeAIOptions> = {}) {\n    this.aiOptions = Object.assign(this.aiOptions, aiOptions);\n    debug('PolyfillChromeAI created', this.aiOptions);\n    this.modelAssetBuffer = fetch(this.aiOptions.modelAssetPath).then(\n      (response) => response.body!.getReader()\n    )!;\n  }\n\n  private modelAssetBuffer: Promise<ReadableStreamDefaultReader>;\n\n  public capabilities = async (): Promise<ChromeAIAssistantCapabilities> => {\n    const defaultOptions = {\n      defaultTemperature: 0.8,\n      defaultTopK: 3,\n      maxTopK: 128,\n    };\n\n    // If browser do not support WebAssembly/WebGPU, return 'no';\n    if (typeof WebAssembly.instantiate !== 'function')\n      return {\n        ...defaultOptions,\n        available: ChromeAICapabilityAvailability.NO,\n      };\n    if (!(<any>navigator).gpu)\n      return {\n        ...defaultOptions,\n        available: ChromeAICapabilityAvailability.NO,\n      };\n\n    // Check if modelAssetBuffer is downloaded, if not, return 'after-download';\n    const isModelAssetBufferReady = await Promise.race([\n      this.modelAssetBuffer,\n      Promise.resolve('sentinel'),\n    ])\n      .then((value) => value === 'sentinel')\n      .catch(() => true);\n\n    return {\n      ...defaultOptions,\n      available: isModelAssetBufferReady\n        ? ChromeAICapabilityAvailability.READILY\n        : ChromeAICapabilityAvailability.AFTER_DOWNLOAD,\n    };\n  };\n\n  public create = async (\n    options?: ChromeAIAssistantCreateOptions\n  ): Promise<ChromeAIAssistant> => {\n    const defaultParams = await this.capabilities();\n    const argv = Object.assign(\n      {\n        temperature: defaultParams.defaultTemperature,\n        topK: defaultParams.defaultTopK,\n      },\n      options\n    );\n    const llm = await LlmInference.createFromOptions(\n      {\n        wasmLoaderPath: this.aiOptions.wasmLoaderPath!,\n        wasmBinaryPath: this.aiOptions.wasmBinaryPath!,\n      },\n      {\n        baseOptions: {\n          modelAssetBuffer: await this.modelAssetBuffer,\n        },\n        temperature: argv.temperature,\n        topK: argv.topK,\n      }\n    );\n    const session = new PolyfillChromeAIAssistant(llm);\n    debug('createSession', options, session);\n    return session;\n  };\n}\n\nexport const polyfillChromeAI = (\n  options?: Partial<PolyfillChromeAIOptions>\n) => {\n  const ai = {\n    assistant: new PolyfillChromeAIAssistantFactory(options),\n  };\n  globalThis.ai = globalThis.ai || ai;\n  globalThis.model = globalThis.model || ai;\n};\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AAAA,EAaE;AAAA,EACA;AAAA,OACK;AAMP,OAAOA,kBAAiB;;;ACjBxB,OAAO,iBAAiB;AAExB,IAAM,QAAQ,YAAY,UAAU;AAE7B,IAAM,sBAAsB;AAC5B,IAAM,qBAAqB;AAE3B,IAAM,WAAN,cAAuB,gBAG5B;AAAA,EACO,YAAY,SAAqC;AACtD,QAAI,SAAS;AACb,QAAI,eAAe;AAEnB,UAAM,QAAQ,MAAM;AAClB,eAAS;AACT,qBAAe;AAAA,IACjB;AAEA,UAAM;AAAA,MACJ,OAAO,CAAC,eAAe;AACrB,cAAM;AACN,YAAI,CAAC,QAAQ,YAAa;AAC1B,gBAAQ,YAAY,iBAAiB,SAAS,MAAM;AAClD,gBAAM,qCAAqC;AAC3C,qBAAW,UAAU;AAAA,QACvB,CAAC;AAAA,MACH;AAAA,MACA,WAAW,CAAC,OAAO,eAAe;AAChC,YAAI,QAAQ,KAAK,SAAS,eAAe;AACvC,yBACE,MAAM,WAAW,mBAAmB,KACpC,CAAC,MAAM,SAAS,kBAAkB;AACpC,kBAAQ,MAAM;AAAA,YACZ,IAAI,OAAO,MAAM,qBAAqB,IAAI;AAAA,YAC1C;AAAA,UACF;AACA,kBAAQ,MAAM,QAAQ,IAAI,OAAO,qBAAqB,KAAK,IAAI,GAAG,EAAE;AAAA,QACtE,OAAO;AACL,yBAAe;AAAA,QACjB;AACA,cAAM,YAAY,MAAM,QAAQ,QAAQ,EAAE;AAC1C,YAAI,aAAc,YAAW,QAAQ,EAAE,MAAM,cAAc,UAAU,CAAC;AACtE,iBAAS;AAAA,MACX;AAAA,MACA,OAAO,CAAC,eAAe;AACrB,mBAAW,QAAQ;AAAA,UACjB,MAAM;AAAA,UACN,cAAc;AAAA,UACd,OAAO,EAAE,kBAAkB,GAAG,cAAc,EAAE;AAAA,QAChD,CAAC;AACD,cAAM,kBAAkB,MAAM;AAAA,MAChC;AAAA,IACF,CAAC;AAAA,EACH;AACF;;;ADpCA,IAAMC,SAAQC,aAAY,UAAU;AAMpC,SAAS,iBACP,SAKQ;AACR,MAAI,OAAO,YAAY,UAAU;AAC/B,WAAO,QAAQ,KAAK;AAAA,EACtB,WAAW,MAAM,QAAQ,OAAO,KAAK,QAAQ,SAAS,GAAG;AACvD,UAAM,CAAC,KAAK,IAAI;AAChB,QAAI,MAAM,SAAS,QAAQ;AACzB,YAAM,IAAI,8BAA8B,EAAE,eAAe,WAAW,CAAC;AAAA,IACvE;AACA,WAAO,MAAM,KAAK,KAAK;AAAA,EACzB,OAAO;AACL,WAAO;AAAA,EACT;AACF;AAEO,IAAM,4BAAN,MAA2D;AAAA,EAUhE,YACE,SACA,UAAgC,CAAC,GACjC;AAZF,SAAS,uBAAuB;AAChC,SAAS,8BAA8B;AACvC,SAAS,UAA+B;AACxC,SAAS,WAAW;AACpB,SAAS,oBAAoB;AAC7B,SAAS,4BAA4B;AAcrC,SAAQ,aAAa,CACnB,YAC+B;AAxEnC;AAyEI,UAAI,GAAC,gBAAW,OAAX,mBAAe,YAAW;AAC7B,cAAM,IAAI,iBAAiB,EAAE,SAAS,sBAAsB,CAAC;AAAA,MAC/D;AACA,UAAI,KAAK,QAAS,QAAO,KAAK;AAE9B,YAAM,MAAM,MAAM,GAAG,UAAU,aAAa;AAE5C,UAAI,IAAI,uCAAsD;AAC5D,cAAM,IAAI,iBAAiB,EAAE,SAAS,2BAA2B,CAAC;AAAA,MACpE;AAEA,WAAK,UAAU;AAAA,QACb,aAAa,IAAI;AAAA,QACjB,MAAM,IAAI;AAAA,SACP,KAAK,UACL;AAGL,WAAK,UAAU,MAAM,GAAG,UAAU,OAAO,KAAK,OAAO;AAErD,MAAAD,OAAM,oBAAoB,KAAK,SAAS,KAAK,OAAO;AACpD,aAAO,KAAK;AAAA,IACd;AAEA,SAAQ,iBAAiB,CAAC,YAAgD;AACxE,UAAI,SAAgC,QAAQ;AAC5C,MAAAA,OAAM,yBAAyB,MAAM;AAErC,UAAI,SAAS;AAEb;AAAA;AAAA,QAEE,QAAQ,gBAAgB,YACxB,OAAO,WAAW,KAClB,OAAO,CAAC,EAAE,SAAS,UACnB,OAAO,CAAC,EAAE,QAAQ,WAAW,KAC7B,OAAO,CAAC,EAAE,QAAQ,CAAC,EAAE,SAAS;AAAA,QAC9B;AACA,kBAAU,OAAO,CAAC,EAAE,QAAQ,CAAC,EAAE;AAAA,MACjC,OAAO;AAEL,YAAI,QAAQ,KAAK,SAAS,eAAe;AACvC,iBAAO,QAAQ;AAAA,YACb,MAAM;AAAA,YACN,SAAS;AAAA;AAAA;AAAA,UACX,CAAC;AAAA,QACH;AAEA,iBAAS,QAAQ,GAAG,QAAQ,OAAO,QAAQ,SAAS,GAAG;AACrD,gBAAM,EAAE,MAAM,QAAQ,IAAI,OAAO,KAAK;AACtC,gBAAM,gBAAgB,iBAAiB,OAAO;AAE9C,kBAAQ,MAAM;AAAA,YACZ,KAAK;AACH,wBAAU,GAAG,aAAa;AAAA;AAC1B;AAAA,YACF,KAAK;AAAA,YACL,KAAK;AACH,wBAAU;AAAA,EAAU,aAAa;AAAA;AACjC;AAAA,YACF,KAAK;AAAA,YACL;AACE,wBAAU;AAAA,EAAS,aAAa;AAAA;AAChC;AAAA,UACJ;AAAA,QACF;AACA,kBAAU;AAAA;AAAA,MACZ;AAEA,MAAAA,OAAM,qBAAqB,MAAM;AACjC,aAAO;AAAA,IACT;AAEA,SAAO,aAAa,CAClB,YAUI;AACJ,MAAAA,OAAM,qBAAqB,OAAO;AAElC,UAAI,CAAC,WAAW,aAAa,EAAE,QAAQ,QAAQ,KAAK,IAAI,IAAI,GAAG;AAC7D,cAAM,IAAI,8BAA8B;AAAA,UACtC,eAAe,GAAG,QAAQ,KAAK,IAAI;AAAA,QACrC,CAAC;AAAA,MACH;AAEA,YAAM,UAAU,MAAM,KAAK,WAAW;AACtC,YAAM,UAAU,KAAK,eAAe,OAAO;AAC3C,UAAI,OAAO,MAAM,QAAQ,OAAO,OAAO;AAEvC,UAAI,QAAQ,KAAK,SAAS,eAAe;AACvC,eAAO,KAAK,QAAQ,IAAI,OAAO,MAAM,qBAAqB,IAAI,GAAG,EAAE;AACnE,eAAO,KAAK,QAAQ,IAAI,OAAO,qBAAqB,KAAK,IAAI,GAAG,EAAE;AAAA,MACpE;AAEA,MAAAA,OAAM,oBAAoB,IAAI;AAE9B,aAAO;AAAA,QACL;AAAA,QACA,cAAc;AAAA,QACd,OAAO,EAAE,cAAc,GAAG,kBAAkB,EAAE;AAAA,QAC9C,SAAS,EAAE,WAAW,QAAQ,QAAQ,aAAa,KAAK,QAAQ;AAAA,MAClE;AAAA,IACF;AAEA,SAAO,WAAW,CAChB,YAMI;AACJ,MAAAA,OAAM,mBAAmB,OAAO;AAEhC,UAAI,CAAC,WAAW,aAAa,EAAE,QAAQ,QAAQ,KAAK,IAAI,IAAI,GAAG;AAC7D,cAAM,IAAI,8BAA8B;AAAA,UACtC,eAAe,GAAG,QAAQ,KAAK,IAAI;AAAA,QACrC,CAAC;AAAA,MACH;AAEA,YAAM,UAAU,MAAM,KAAK,WAAW;AACtC,YAAM,UAAU,KAAK,eAAe,OAAO;AAC3C,YAAM,eAAe,QAAQ,gBAAgB,OAAO;AACpD,YAAM,kBAAkB,IAAI,SAAS,OAAO;AAC5C,YAAM,SAAS,aAAa,YAAY,eAAe;AAEvD,aAAO;AAAA,QACL;AAAA,QACA,SAAS,EAAE,WAAW,QAAQ,QAAQ,aAAa,KAAK,QAAQ;AAAA,MAClE;AAAA,IACF;AAnJE,SAAK,UAAU;AACf,SAAK,UAAU;AACf,IAAAA,OAAM,SAAS,KAAK,OAAO;AAAA,EAC7B;AAiJF;;;AEnNA,SAAS,oBAAoB;AA4CtB,IAAM,yBAAN,MAAiE;AAAA,EAoB/D,YAAY,WAA2C,CAAC,GAAG;AAnBlE,SAAS,uBAAuB;AAChC,SAAS,WAAW;AACpB,SAAS,UAAkB;AAC3B,SAAS,wBAAwB;AACjC,SAAS,uBAAuB;AAEhC,SAAQ,WAA2C;AAAA,MACjD,gBACE;AAAA,MACF,gBACE;AAAA,MACF,gBACE;AAAA,MACF,aAAa;AAAA,MACb,UAAU;AAAA,IACZ;AAYA,SAAU,kBAAkB,MAAmC;AAC7D,aAAO,aAAa;AAAA,QAClB;AAAA,UACE,gBAAgB,KAAK,SAAS;AAAA,UAC9B,gBAAgB,KAAK,SAAS;AAAA,QAChC;AAAA,QACA;AAAA,UACE,aAAa;AAAA,YACX,kBAAkB,MAAM,KAAK;AAAA,YAC7B,UAAU,KAAK,SAAS;AAAA,UAC1B;AAAA,UACA,aAAa,KAAK,SAAS;AAAA,UAC3B,UAAU,KAAK,SAAS;AAAA,QAC1B;AAAA,MACF;AAAA,IACF;AAEA,SAAO,UAAU,CAAO,YAMlB;AAEJ,YAAM,WAAW,MAAM,KAAK;AAC5B,YAAM,aAAa,QAAQ,OAAO,IAAI,CAAC,SAAS;AAnGpD;AAoGM,cAAM,iBAAiB,SAAS,MAAM,IAAI;AAC1C,cAAM,CAAC,SAAS,IAAI,eAAe;AACnC,gBAAO,4CAAW,mBAAX,YAA6B,CAAC;AAAA,MACvC,CAAC;AACD,aAAO,EAAE,WAAW;AAAA,IACtB;AAvCE,SAAK,WAAW,kCAAK,KAAK,WAAa;AACvC,SAAK,mBAAmB,MAAM,KAAK,SAAS,cAAe,EAAE;AAAA,MAC3D,CAAC,aAAa,SAAS,KAAM,UAAU;AAAA,IACzC;AACA,SAAK,eAAe,KAAK,gBAAgB;AAAA,EAC3C;AAmCF;;;ACjGA,OAAOE,kBAAiB;AAExB,IAAMC,SAAQD,aAAY,UAAU;AAe7B,SAAS,SAAS,UAAmB,QAAQ,WAAoB,CAAC,GAAG;AAC1E,EAAAC,OAAM,mBAAmB,SAAS,QAAQ;AAC1C,MAAI,YAAY,aAAa;AAC3B,WAAO,IAAI;AAAA,MACT;AAAA,IACF;AAAA,EACF;AACA,SAAO,IAAI;AAAA,IACT;AAAA,IACA;AAAA,EACF;AACF;AAGA,SAAS,YAAY,CAAC,WAA2C,CAAC,MAChE,IAAI,uBAAuB,QAAQ;;;ACzCrC,SAAS,oBAAsC;AAS/C,OAAOC,kBAAiB;AAExB,IAAMC,SAAQC,aAAY,mBAAmB;AAE7C,IAAM,4BAAN,MAA6D;AAAA,EACpD,YAAoB,KAAmB;AAAnB;AAI3B,SAAO,SAAS,CAAO,WAAoC;AACzD,YAAM,WAAW,MAAM,KAAK,IAAI,iBAAiB,MAAM;AACvD,MAAAD,OAAM,UAAU,QAAQ,QAAQ;AAChC,aAAO;AAAA,IACT;AAEA,SAAO,kBAAkB,CAAC,WAA2C;AACnE,MAAAA,OAAM,mBAAmB,MAAM;AAC/B,YAAM,SAAS,IAAI,eAAuB;AAAA,QACxC,OAAO,CAAC,eAAe;AACrB,gBAAM,WAA6B,CACjC,eACA,SACG;AACH,uBAAW,QAAQ,aAAa;AAChC,gBAAI,MAAM;AACR,yBAAW,MAAM;AAAA,YACnB;AAAA,UACF;AACA,eAAK,IAAI,iBAAiB,QAAQ,QAAQ;AAAA,QAC5C;AAAA,QACA,QAAQ,CAAC,WAAW;AAClB,kBAAQ,KAAK,wBAAwB,MAAM;AAAA,QAC7C;AAAA,MACF,CAAC;AACD,MAAAA,OAAM,mBAAmB,MAAM;AAC/B,aAAO;AAAA,IACT;AAEA,SAAO,UAAU,MAAS;AAAG,kBAAK,IAAI,MAAM;AAAA;AAhC1C,IAAAA,OAAM,qCAAqC,GAAG;AAAA,EAChD;AAgCF;AAKO,IAAM,mCAAN,MAEP;AAAA,EAWS,YAAY,YAA8C,CAAC,GAAG;AAVrE,SAAQ,YAAqC;AAAA,MAC3C,gBACE;AAAA,MACF,gBACE;AAAA;AAAA,MAEF,gBACE;AAAA,IACJ;AAYA,SAAO,eAAe,MAAoD;AACxE,YAAM,iBAAiB;AAAA,QACrB,oBAAoB;AAAA,QACpB,aAAa;AAAA,QACb,SAAS;AAAA,MACX;AAGA,UAAI,OAAO,YAAY,gBAAgB;AACrC,eAAO,iCACF,iBADE;AAAA,UAEL;AAAA,QACF;AACF,UAAI,CAAO,UAAW;AACpB,eAAO,iCACF,iBADE;AAAA,UAEL;AAAA,QACF;AAGF,YAAM,0BAA0B,MAAM,QAAQ,KAAK;AAAA,QACjD,KAAK;AAAA,QACL,QAAQ,QAAQ,UAAU;AAAA,MAC5B,CAAC,EACE,KAAK,CAAC,UAAU,UAAU,UAAU,EACpC,MAAM,MAAM,IAAI;AAEnB,aAAO,iCACF,iBADE;AAAA,QAEL,WAAW;AAAA,MAGb;AAAA,IACF;AAEA,SAAO,SAAS,CACd,YAC+B;AAC/B,YAAM,gBAAgB,MAAM,KAAK,aAAa;AAC9C,YAAM,OAAO,OAAO;AAAA,QAClB;AAAA,UACE,aAAa,cAAc;AAAA,UAC3B,MAAM,cAAc;AAAA,QACtB;AAAA,QACA;AAAA,MACF;AACA,YAAM,MAAM,MAAM,aAAa;AAAA,QAC7B;AAAA,UACE,gBAAgB,KAAK,UAAU;AAAA,UAC/B,gBAAgB,KAAK,UAAU;AAAA,QACjC;AAAA,QACA;AAAA,UACE,aAAa;AAAA,YACX,kBAAkB,MAAM,KAAK;AAAA,UAC/B;AAAA,UACA,aAAa,KAAK;AAAA,UAClB,MAAM,KAAK;AAAA,QACb;AAAA,MACF;AACA,YAAM,UAAU,IAAI,0BAA0B,GAAG;AACjD,MAAAA,OAAM,iBAAiB,SAAS,OAAO;AACvC,aAAO;AAAA,IACT;AAvEE,SAAK,YAAY,OAAO,OAAO,KAAK,WAAW,SAAS;AACxD,IAAAA,OAAM,4BAA4B,KAAK,SAAS;AAChD,SAAK,mBAAmB,MAAM,KAAK,UAAU,cAAc,EAAE;AAAA,MAC3D,CAAC,aAAa,SAAS,KAAM,UAAU;AAAA,IACzC;AAAA,EACF;AAmEF;AAEO,IAAM,mBAAmB,CAC9B,YACG;AACH,QAAME,MAAK;AAAA,IACT,WAAW,IAAI,iCAAiC,OAAO;AAAA,EACzD;AACA,aAAW,KAAK,WAAW,MAAMA;AACjC,aAAW,QAAQ,WAAW,SAASA;AACzC;","names":["createDebug","debug","createDebug","createDebug","debug","createDebug","debug","createDebug","ai"]}