var __defProp = Object.defineProperty;
var __defProps = Object.defineProperties;
var __getOwnPropDescs = Object.getOwnPropertyDescriptors;
var __getOwnPropSymbols = Object.getOwnPropertySymbols;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __propIsEnum = Object.prototype.propertyIsEnumerable;
var __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;
var __spreadValues = (a, b) => {
  for (var prop in b || (b = {}))
    if (__hasOwnProp.call(b, prop))
      __defNormalProp(a, prop, b[prop]);
  if (__getOwnPropSymbols)
    for (var prop of __getOwnPropSymbols(b)) {
      if (__propIsEnum.call(b, prop))
        __defNormalProp(a, prop, b[prop]);
    }
  return a;
};
var __spreadProps = (a, b) => __defProps(a, __getOwnPropDescs(b));
var __async = (__this, __arguments, generator) => {
  return new Promise((resolve, reject) => {
    var fulfilled = (value) => {
      try {
        step(generator.next(value));
      } catch (e) {
        reject(e);
      }
    };
    var rejected = (value) => {
      try {
        step(generator.throw(value));
      } catch (e) {
        reject(e);
      }
    };
    var step = (x) => x.done ? resolve(x.value) : Promise.resolve(x.value).then(fulfilled, rejected);
    step((generator = generator.apply(__this, __arguments)).next());
  });
};

// src/language-model.ts
import {
  LoadSettingError,
  UnsupportedFunctionalityError
} from "@ai-sdk/provider";
import createDebug2 from "debug";

// src/stream-ai.ts
import createDebug from "debug";
var debug = createDebug("chromeai");
var objectStartSequence = " ```json\n";
var objectStopSequence = "\n```";
var StreamAI = class extends TransformStream {
  constructor(options) {
    let buffer = "";
    let transforming = false;
    const reset = () => {
      buffer = "";
      transforming = false;
    };
    super({
      start: (controller) => {
        reset();
        if (!options.abortSignal) return;
        options.abortSignal.addEventListener("abort", () => {
          debug("streamText terminate by abortSignal");
          controller.terminate();
        });
      },
      transform: (chunk, controller) => {
        if (options.mode.type === "object-json") {
          transforming = chunk.startsWith(objectStartSequence) && !chunk.endsWith(objectStopSequence);
          chunk = chunk.replace(
            new RegExp("^" + objectStartSequence, "ig"),
            ""
          );
          chunk = chunk.replace(new RegExp(objectStopSequence + "$", "ig"), "");
        } else {
          transforming = true;
        }
        const textDelta = chunk.replace(buffer, "");
        if (transforming) controller.enqueue({ type: "text-delta", textDelta });
        buffer = chunk;
      },
      flush: (controller) => {
        controller.enqueue({
          type: "finish",
          finishReason: "stop",
          usage: { completionTokens: 0, promptTokens: 0 }
        });
        debug("stream result:", buffer);
      }
    });
  }
};

// src/language-model.ts
var debug2 = createDebug2("chromeai");
function getStringContent(content) {
  if (typeof content === "string") {
    return content.trim();
  } else if (Array.isArray(content) && content.length > 0) {
    const [first] = content;
    if (first.type !== "text") {
      throw new UnsupportedFunctionalityError({ functionality: "toolCall" });
    }
    return first.text.trim();
  } else {
    return "";
  }
}
var ChromeAIChatLanguageModel = class {
  constructor(modelId, options = {}) {
    this.specificationVersion = "v1";
    this.defaultObjectGenerationMode = "json";
    this.modelId = "text";
    this.provider = "gemini-nano";
    this.supportsImageUrls = false;
    this.supportsStructuredOutputs = false;
    this.getSession = (options) => __async(this, null, function* () {
      var _a;
      if (!((_a = globalThis.ai) == null ? void 0 : _a.assistant)) {
        throw new LoadSettingError({ message: "Browser not support" });
      }
      if (this.session) return this.session;
      const cap = yield ai.assistant.capabilities();
      if (cap.available !== "readily" /* READILY */) {
        throw new LoadSettingError({ message: "Built-in model not ready" });
      }
      this.options = __spreadValues(__spreadValues({
        temperature: cap.defaultTemperature,
        topK: cap.defaultTopK
      }, this.options), options);
      this.session = yield ai.assistant.create(this.options);
      debug2("session created:", this.session, this.options);
      return this.session;
    });
    this.formatMessages = (options) => {
      let prompt = options.prompt;
      debug2("before format prompt:", prompt);
      let result = "";
      if (
        // When the user supplied a prompt input, we don't transform it
        options.inputFormat === "prompt" && prompt.length === 1 && prompt[0].role === "user" && prompt[0].content.length === 1 && prompt[0].content[0].type === "text"
      ) {
        result += prompt[0].content[0].text;
      } else {
        if (options.mode.type === "object-json") {
          prompt.unshift({
            role: "system",
            content: `Throughout our conversation, always start your responses with "{" and end with "}", ensuring the output is a concise JSON object and strictly avoid including any comments, notes, explanations, or examples in your output.
For instance, if the JSON schema is {"type":"object","properties":{"someKey":{"type":"string"}},"required":["someKey"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"}, your response should immediately begin with "{" and strictly end with "}", following the format: {"someKey": "someValue"}.
Adhere to this format for all queries moving forward.`
          });
        }
        for (let index = 0; index < prompt.length; index += 1) {
          const { role, content } = prompt[index];
          const contentString = getStringContent(content);
          switch (role) {
            case "system":
              result += `${contentString}
`;
              break;
            case "assistant":
            case "tool":
              result += `model
${contentString}
`;
              break;
            case "user":
            default:
              result += `user
${contentString}
`;
              break;
          }
        }
        result += `model
`;
      }
      debug2("formated message:", result);
      return result;
    };
    this.doGenerate = (options) => __async(this, null, function* () {
      debug2("generate options:", options);
      if (["regular", "object-json"].indexOf(options.mode.type) < 0) {
        throw new UnsupportedFunctionalityError({
          functionality: `${options.mode.type} mode`
        });
      }
      const session = yield this.getSession();
      const message = this.formatMessages(options);
      let text = yield session.prompt(message);
      if (options.mode.type === "object-json") {
        text = text.replace(new RegExp("^" + objectStartSequence, "ig"), "");
        text = text.replace(new RegExp(objectStopSequence + "$", "ig"), "");
      }
      debug2("generate result:", text);
      return {
        text,
        finishReason: "stop",
        usage: { promptTokens: 0, completionTokens: 0 },
        rawCall: { rawPrompt: options.prompt, rawSettings: this.options }
      };
    });
    this.doStream = (options) => __async(this, null, function* () {
      debug2("stream options:", options);
      if (["regular", "object-json"].indexOf(options.mode.type) < 0) {
        throw new UnsupportedFunctionalityError({
          functionality: `${options.mode.type} mode`
        });
      }
      const session = yield this.getSession();
      const message = this.formatMessages(options);
      const promptStream = session.promptStreaming(message);
      const transformStream = new StreamAI(options);
      const stream = promptStream.pipeThrough(transformStream);
      return {
        stream,
        rawCall: { rawPrompt: options.prompt, rawSettings: this.options }
      };
    });
    this.modelId = modelId;
    this.options = options;
    debug2("init:", this.modelId);
  }
};

// src/embedding-model.ts
import { TextEmbedder } from "@mediapipe/tasks-text";
var ChromeAIEmbeddingModel = class {
  constructor(settings = {}) {
    this.specificationVersion = "v1";
    this.provider = "google-mediapipe";
    this.modelId = "embedding";
    this.supportsParallelCalls = true;
    this.maxEmbeddingsPerCall = void 0;
    this.settings = {
      wasmLoaderPath: "https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/text_wasm_internal.js",
      wasmBinaryPath: "https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/text_wasm_internal.wasm",
      modelAssetPath: "https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/universal_sentence_encoder.tflite",
      l2Normalize: false,
      quantize: false
    };
    this.getTextEmbedder = () => __async(this, null, function* () {
      return TextEmbedder.createFromOptions(
        {
          wasmBinaryPath: this.settings.wasmBinaryPath,
          wasmLoaderPath: this.settings.wasmLoaderPath
        },
        {
          baseOptions: {
            modelAssetBuffer: yield this.modelAssetBuffer,
            delegate: this.settings.delegate
          },
          l2Normalize: this.settings.l2Normalize,
          quantize: this.settings.quantize
        }
      );
    });
    this.doEmbed = (options) => __async(this, null, function* () {
      const embedder = yield this.textEmbedder;
      const embeddings = options.values.map((text) => {
        var _a;
        const embedderResult = embedder.embed(text);
        const [embedding] = embedderResult.embeddings;
        return (_a = embedding == null ? void 0 : embedding.floatEmbedding) != null ? _a : [];
      });
      return { embeddings };
    });
    this.settings = __spreadValues(__spreadValues({}, this.settings), settings);
    this.modelAssetBuffer = fetch(this.settings.modelAssetPath).then(
      (response) => response.body.getReader()
    );
    this.textEmbedder = this.getTextEmbedder();
  }
};

// src/chromeai.ts
import createDebug3 from "debug";
var debug3 = createDebug3("chromeai");
function chromeai(modelId = "text", settings = {}) {
  debug3("create instance", modelId, settings);
  if (modelId === "embedding") {
    return new ChromeAIEmbeddingModel(
      settings
    );
  }
  return new ChromeAIChatLanguageModel(
    modelId,
    settings
  );
}
chromeai.embedding = (settings = {}) => new ChromeAIEmbeddingModel(settings);

// src/polyfill/session.ts
import { LlmInference } from "@mediapipe/tasks-genai";
import createDebug4 from "debug";
var debug4 = createDebug4("chromeai:polyfill");
var PolyfillChromeAIAssistant = class {
  constructor(llm) {
    this.llm = llm;
    this.prompt = (prompt) => __async(this, null, function* () {
      const response = yield this.llm.generateResponse(prompt);
      debug4("prompt", prompt, response);
      return response;
    });
    this.promptStreaming = (prompt) => {
      debug4("promptStreaming", prompt);
      const stream = new ReadableStream({
        start: (controller) => {
          const listener = (partialResult, done) => {
            controller.enqueue(partialResult);
            if (done) {
              controller.close();
            }
          };
          this.llm.generateResponse(prompt, listener);
        },
        cancel: (reason) => {
          console.warn("stream text canceled", reason);
        }
      });
      debug4("promptStreaming", prompt);
      return stream;
    };
    this.destroy = () => __async(this, null, function* () {
      return this.llm.close();
    });
    debug4("PolyfillChromeAIAssistant created", llm);
  }
};
var PolyfillChromeAIAssistantFactory = class {
  constructor(aiOptions = {}) {
    this.aiOptions = {
      wasmBinaryPath: "https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/genai_wasm_internal.wasm",
      wasmLoaderPath: "https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/genai_wasm_internal.js",
      // About 1.78GB, should cache by browser
      modelAssetPath: "https://pub-ddcfe353995744e89b8002f16bf98575.r2.dev/gemini-nano-it-chrome-128.bin"
    };
    this.capabilities = () => __async(this, null, function* () {
      const defaultOptions = {
        defaultTemperature: 0.8,
        defaultTopK: 3,
        maxTopK: 128
      };
      if (typeof WebAssembly.instantiate !== "function")
        return __spreadProps(__spreadValues({}, defaultOptions), {
          available: "no" /* NO */
        });
      if (!navigator.gpu)
        return __spreadProps(__spreadValues({}, defaultOptions), {
          available: "no" /* NO */
        });
      const isModelAssetBufferReady = yield Promise.race([
        this.modelAssetBuffer,
        Promise.resolve("sentinel")
      ]).then((value) => value === "sentinel").catch(() => true);
      return __spreadProps(__spreadValues({}, defaultOptions), {
        available: isModelAssetBufferReady ? "readily" /* READILY */ : "after-download" /* AFTER_DOWNLOAD */
      });
    });
    this.create = (options) => __async(this, null, function* () {
      const defaultParams = yield this.capabilities();
      const argv = Object.assign(
        {
          temperature: defaultParams.defaultTemperature,
          topK: defaultParams.defaultTopK
        },
        options
      );
      const llm = yield LlmInference.createFromOptions(
        {
          wasmLoaderPath: this.aiOptions.wasmLoaderPath,
          wasmBinaryPath: this.aiOptions.wasmBinaryPath
        },
        {
          baseOptions: {
            modelAssetBuffer: yield this.modelAssetBuffer
          },
          temperature: argv.temperature,
          topK: argv.topK
        }
      );
      const session = new PolyfillChromeAIAssistant(llm);
      debug4("createSession", options, session);
      return session;
    });
    this.aiOptions = Object.assign(this.aiOptions, aiOptions);
    debug4("PolyfillChromeAI created", this.aiOptions);
    this.modelAssetBuffer = fetch(this.aiOptions.modelAssetPath).then(
      (response) => response.body.getReader()
    );
  }
};
var polyfillChromeAI = (options) => {
  const ai2 = {
    assistant: new PolyfillChromeAIAssistantFactory(options)
  };
  globalThis.ai = globalThis.ai || ai2;
  globalThis.model = globalThis.model || ai2;
};
export {
  ChromeAIChatLanguageModel,
  ChromeAIEmbeddingModel,
  PolyfillChromeAIAssistantFactory,
  chromeai,
  polyfillChromeAI
};
//# sourceMappingURL=index.mjs.map